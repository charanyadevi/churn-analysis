# -*- coding: utf-8 -*-
"""churn1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mZWYYf7IGQUyvZuVtyYrNMJveQI7HdXB

# **EDA**
"""

#Import libraries
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)
import plotly.express as px
import matplotlib.pyplot as plt

#Read the dataset
df = pd.read_csv("/content/drive/MyDrive/Tel_Customer_Churn_Dataset.csv")
df

def dataoveriew(df, message):
    print(f'{message}:n')
    print('Number of rows: ', df.shape[0])
    print("nNumber of features:", df.shape[1])
    print("nData Features:")
    print(df.columns.tolist())
    print("nMissing values:", df.isnull().sum().values.sum())
    print("nUnique values:")
    print(df.nunique())

dataoveriew(df, 'Overview of the dataset')

target_instance = df["Churn"].value_counts().to_frame()
target_instance = target_instance.reset_index()
target_instance = target_instance.rename(columns={'index': 'Category'})
fig = px.pie(target_instance, values='Churn', names='Category', color_discrete_sequence=["green", "red"],
             title='Distribution of Churn')
fig.show()

#Defining bar chart function
def bar(feature, df=df ):
    #Groupby the categorical feature
    temp_df = df.groupby([feature, 'Churn']).size().reset_index()
    temp_df = temp_df.rename(columns={0:'Count'})
    #Calculate the value counts of each distribution and it's corresponding Percentages
    value_counts_df = df[feature].value_counts().to_frame().reset_index()
    categories = [cat[1][0] for cat in value_counts_df.iterrows()]
    #Calculate the value counts of each distribution and it's corresponding Percentages
    num_list = [num[1][1] for num in value_counts_df.iterrows()]
    div_list = [element / sum(num_list) for element in num_list]
    percentage = [round(element * 100,1) for element in div_list]
    #Defining string formatting for graph
    #Numeric section
    def num_format(list_instance):
        formatted_str = ''
        for index,num in enumerate(list_instance):
            if index < len(list_instance)-2:
                formatted_str=formatted_str+f'{num}%, ' #append to empty string(formatted_str)
            elif index == len(list_instance)-2:
                formatted_str=formatted_str+f'{num}% & '
            else:
                formatted_str=formatted_str+f'{num}%'
        return formatted_str
    #Categorical section
    def str_format(list_instance):
        formatted_str = ''
        for index, cat in enumerate(list_instance):
            if index < len(list_instance)-2:
                formatted_str=formatted_str+f'{cat}, '
            elif index == len(list_instance)-2:
                formatted_str=formatted_str+f'{cat} & '
            else:
                formatted_str=formatted_str+f'{cat}'
        return formatted_str
    #Running the formatting functions
    num_str = num_format(percentage)
    cat_str = str_format(categories)

    #Setting graph framework
    fig = px.bar(temp_df, x=feature, y='Count', color='Churn', title=f'Churn rate by {feature}', barmode="group", color_discrete_sequence=["green", "red"])
    fig.add_annotation(
                text=f'Value count of distribution of {cat_str} are<br>{num_str} percentage respectively.',
                align='left',
                showarrow=False,
                xref='paper',
                yref='paper',
                x=1.4,
                y=1.3,
                bordercolor='black',
                borderwidth=1)
    fig.update_layout(
        # margin space for the annotations on the right
        margin=dict(r=400),
    )

    return fig.show()

##analysing demographic attributes
#Gender feature plot
bar('gender')
#SeniorCitizen feature plot
df.loc[df.SeniorCitizen==0,'SeniorCitizen'] = "No"
df.loc[df.SeniorCitizen==1,'SeniorCitizen'] = "Yes"
bar('SeniorCitizen')
#Partner feature plot
bar('Partner')
#Dependents feature plot
bar('Dependents')

##analysing services attributes
bar('PhoneService')
bar('MultipleLines')
bar('InternetService')
bar('OnlineSecurity')
bar('OnlineBackup')
bar('DeviceProtection')
bar('TechSupport')
bar('StreamingTV')
bar('StreamingMovies')

##analysing payment features
bar('Contract')
bar('PaperlessBilling')
bar('PaymentMethod')

#exploring numerical data
df.dtypes

try:
    df['TotalCharges'] = df['TotalCharges'].astype(float)
except ValueError as ve:
    print (ve)

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'],errors='coerce')
#Fill the missing values with with the median value
df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median())

# Defining the histogram plotting function
def hist(feature):
    group_df = df.groupby([feature, 'Churn']).size().reset_index()
    group_df = group_df.rename(columns={0: 'Count'})
    fig = px.histogram(group_df, x=feature, y='Count', color='Churn', marginal='box', title=f'Churn rate frequency to {feature} distribution', color_discrete_sequence=["green", "red"])
    fig.show()

hist('tenure')
hist('MonthlyCharges')
hist('TotalCharges')

#Create an empty dataframe
bin_df = pd.DataFrame()

#Update the binning dataframe
bin_df['tenure_bins'] =  pd.qcut(df['tenure'], q=3, labels= ['low', 'medium', 'high'])
bin_df['MonthlyCharges_bins'] =  pd.qcut(df['MonthlyCharges'], q=3, labels= ['low', 'medium', 'high'])
bin_df['TotalCharges_bins'] =  pd.qcut(df['TotalCharges'], q=3, labels= ['low', 'medium', 'high'])
bin_df['Churn'] = df['Churn']

#Plot the bar chart of the binned variables
bar('tenure_bins', bin_df)
bar('MonthlyCharges_bins', bin_df)
bar('TotalCharges_bins', bin_df)

##Data preprocessing:
# The customerID column isnt useful as the feature is used for identification of customers.
df.drop(["customerID"],axis=1,inplace = True)

# Encode categorical features

#Defining the map function
def binary_map(feature):
    return feature.map({'Yes':1, 'No':0})

## Encoding target feature
df['Churn'] = df[['Churn']].apply(binary_map)

# Encoding gender category
df['gender'] = df['gender'].map({'Male':1, 'Female':0})

#Encoding other binary category
binary_list = ['SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']
df[binary_list] = df[binary_list].apply(binary_map)

#Encoding the other categoric features with more than two categories
df = pd.get_dummies(df, drop_first=True)

#correlation
corr = df.corr()

fig = px.imshow(corr,width=1000, height=1000)
fig.show()

#generalized linear model:
import statsmodels.api as sm
import statsmodels.formula.api as smf

#Change variable name separators to '_'
all_columns = [column.replace(" ", "_").replace("(", "_").replace(")", "_").replace("-", "_") for column in df.columns]

#Effect the change to the dataframe column names
df.columns = all_columns

#Prepare it for the GLM formula
glm_columns = [e for e in all_columns if e not in ['customerID', 'Churn']]
glm_columns = ' + '.join(map(str, glm_columns))

#Fiting it to the Generalized Linear Model
glm_model = smf.glm(formula=f'Churn ~ {glm_columns}', data=df, family=sm.families.Binomial())
res = glm_model.fit()
print(res.summary())

#exponential coefficient values:
np.exp(res.params)

#feature scaling
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
df['tenure'] = sc.fit_transform(df[['tenure']])
df['MonthlyCharges'] = sc.fit_transform(df[['MonthlyCharges']])
df['TotalCharges'] = sc.fit_transform(df[['TotalCharges']])

##building ML models
# Import Machine learning algorithms
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

#Import metric for performance evaluation
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

#Split data into train and test sets
from sklearn.model_selection import train_test_split
X = df.drop('Churn', axis=1)
y = df['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)

#Defining the modelling function
def modeling(alg, alg_name, params={}):
    model = alg(**params) #Instantiating the algorithm class and unpacking parameters if any
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    #Performance evaluation
    def print_scores(alg, y_true, y_pred):
        print(alg_name)
        acc_score = accuracy_score(y_true, y_pred)
        print("accuracy: ",acc_score)
        pre_score = precision_score(y_true, y_pred)
        print("precision: ",pre_score)
        rec_score = recall_score(y_true, y_pred)
        print("recall: ",rec_score)
        f_score = f1_score(y_true, y_pred, average='weighted')
        print("f1_score: ",f_score)

    print_scores(alg, y_test, y_pred)
    return model

# Running logistic regression model
log_model = modeling(LogisticRegression, 'Logistic Regression')

# Feature selection to improve model building
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
log = LogisticRegression()
rfecv = RFECV(estimator=log, cv=StratifiedKFold(10, random_state=50, shuffle=True), scoring="accuracy")
rfecv.fit(X, y)

plt.figure(figsize=(8, 6))
plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])
plt.grid()
plt.xticks(range(1, X.shape[1] + 1))
plt.xlabel("Number of Selected Features")
plt.ylabel("Cross-Validation Score")
plt.title("RFECV Results")
plt.show()

#Saving dataframe with optimal features
X_rfe = X.iloc[:, rfecv.support_]

# Overview of the optimal features in comparison with the initial dataframe
print("X dimension: {}".format(X.shape))
print("X column list:", X.columns.tolist())
print("X_rfe dimension: {}".format(X_rfe.shape))
print("X_rfe column list:", X_rfe.columns.tolist())

"""# **LOGISTIC MODEL:**"""

# Splitting data with optimal features
X_train, X_test, y_train, y_test = train_test_split(X_rfe, y, test_size=0.3, random_state=50)

# Running logistic regression model
log_model = modeling(LogisticRegression, 'Logistic Regression Classification')

"""# **SVC MODEL:**"""

##SVC
svc_model = modeling(SVC, 'SVC Classification')

"""# **RANDOM FOREST MODEL:**"""

#Random forest
rf_model = modeling(RandomForestClassifier, "Random Forest Classification")

"""# **DECISION TREE MODEL:**"""

#Decision tree
dt_model = modeling(DecisionTreeClassifier, "Decision Tree Classification")

"""# **NAIVE BAYES MODEL:**"""

#Naive bayes
nb_model = modeling(GaussianNB, "Naive Bayes Classification")

"""## **Tuning Logistic regression model:**"""

## Improve best model by hyperparameter tuning
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Define the logistic regression model
model = LogisticRegression()

# Define evaluation using RepeatedStratifiedKFold
from sklearn.model_selection import RepeatedStratifiedKFold
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

# Define the search space for hyperparameters
param_grid = {
    'solver': ['newton-cg', 'lbfgs', 'liblinear'],
    'penalty': ['l2'],  # Focus on L2 penalty
    'C': [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100, 1000]  # Different C values
}

# Create a GridSearchCV object
search = GridSearchCV(model, param_grid, scoring='accuracy', cv=cv, n_jobs=-1)

# Execute the hyperparameter search
result = search.fit(X_rfe, y)

# Get the best hyperparameters
params = result.best_params_

# Create and fit the improved logistic regression model
logistic_model = LogisticRegression(**params)
logistic_model.fit(X_rfe, y)

# Optionally, you can also print the best score and best hyperparameters
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

"""# **Tuning Random Forest model:**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the Random Forest model
model = RandomForestClassifier(random_state=1)

# Define evaluation using RepeatedStratifiedKFold
from sklearn.model_selection import RepeatedStratifiedKFold
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

# Define the search space for hyperparameters
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# Create a GridSearchCV object
search = GridSearchCV(model, param_grid, scoring='accuracy', cv=cv, n_jobs=-1)

# Execute the hyperparameter search
result = search.fit(X_rfe, y)

# Get the best hyperparameters
params = result.best_params_

# Create and fit the improved Random Forest model
random_forest_model = RandomForestClassifier(random_state=1, **params)
random_forest_model.fit(X_rfe, y)

# Optionally, you can also print the best score and best hyperparameters
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

"""# **Tuning Naive bayes model:**"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV

# Define the Multinomial Naive Bayes model
model = MultinomialNB()

# Define evaluation using RepeatedStratifiedKFold
from sklearn.model_selection import RepeatedStratifiedKFold
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

# Define the search space for hyperparameters
param_grid = {
    'alpha': [0.1, 1.0, 2.0],  # You can adjust the alpha (smoothing parameter)
}

# Create a GridSearchCV object
search = GridSearchCV(model, param_grid, scoring='accuracy', cv=cv, n_jobs=-1)

# Execute the hyperparameter search
result = search.fit(X_rfe, y)

# Get the best hyperparameters
params = result.best_params_

# Create and fit the improved Multinomial Naive Bayes model
naive_bayes_model = MultinomialNB(**params)
naive_bayes_model.fit(X_rfe, y)

# Optionally, you can also print the best score and best hyperparameters
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

"""# **Tuning Decision Tree Classifier:**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Define the Decision Tree model
model = DecisionTreeClassifier(random_state=1)

# Define evaluation using RepeatedStratifiedKFold
from sklearn.model_selection import RepeatedStratifiedKFold
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

# Define the search space for hyperparameters
param_grid = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy'],
}

# Create a GridSearchCV object
search = GridSearchCV(model, param_grid, scoring='accuracy', cv=cv, n_jobs=-1)

# Execute the hyperparameter search
result = search.fit(X_rfe, y)

# Get the best hyperparameters
params = result.best_params_

# Create and fit the improved Decision Tree model
decision_tree_model = DecisionTreeClassifier(random_state=1, **params)
decision_tree_model.fit(X_rfe, y)

# Optionally, you can also print the best score and best hyperparameters
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)